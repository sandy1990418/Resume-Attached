{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "import pandas as pd\n",
    "import time\n",
    "import os \n",
    "\n",
    "# save env\n",
    "import pickle  \n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import csv\n",
    "from pandas import Series,DataFrame\n",
    "import unicodecsv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "import pyautogui\n",
    "from selenium.webdriver.common.action_chains import ActionChains #move_to_element\n",
    "from selenium.common.exceptions import StaleElementReferenceException #error > element is not attached to the page document\n",
    "from selenium.common.exceptions import NoSuchElementException #點完所有按鈕時\n",
    "from  itertools import permutations\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pprint\n",
    "import gensim\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "\n",
    "# %%\n",
    "company_list = pd.read_excel('D:\\\\Stata\\\\stata 桌面\\\\stata系列\\\\計畫18_又予\\\\20210104新計畫\\\\20210128_重新整理\\\\以sp1500為主(新的)\\\\stata\\\\10-K\\\\sp1500\\\\sp1500.xls')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## RULE\n",
    "\n",
    "# %%\n",
    "############################################  keywords  ############################################\n",
    "##  keyword list 1 \n",
    "keywords = [\"essential\",\"key\",\"core\",\"important\",\"skill\",\"skilled\",\"skillful\",\"trained\",\"experienced\",\"talented\",\"qualified\",\"quality\",\"effective\"]\n",
    "\n",
    "##  keyword list 2\n",
    "keywords2 = [\"worker\",\"employee\",\"eligible employee\",\"personnel\",\"colleague\",\"team\",\"member\",\"individual\",\"people\",\"specialist\",\"labor\",\"staff\",\"professional\",\n",
    "\"workforce\",\"professional staff\",\"scientist\",\"technician\",\"research profession\",\"scientific personnel\",\"talent\",\"labor\"]\n",
    "\n",
    "##  keyword list 3\n",
    "keywords3 = ['recruit and retain','recruit or retain','recruit and retaining','recruit or retaining',\n",
    "            'recruiting and retain','recruiting or retain','recruiting and retaining',\n",
    "            'recruiting or retaining','recruit and retain','attract or retain','attract and retain',\n",
    "            'attract and retaining','attract or retaining','attracting and retain','attracting or retain',\n",
    "            'attracting and retaining','attracting or retaining','retain and recruit','retain or recruit','retaining and recruit',\n",
    "            'retaining or recruit','retain and recruiting','retain or recruiting','retaining and recruiting',\n",
    "            'retaining or recruiting','retain and attract','retain or attract','retaining and attract',\n",
    "            'retaining or attract','retain and attracting','retain or attracting','retaining and attracting',\n",
    "            'retaining or attracting']\n",
    "\n",
    "keywords4 = [\"recruit\",\"attract\",\"retain\",\"recruiting\",\"attracting\",\"retaining\",\"attraction\",\"retention\",\"associate\",\"recruitment\",\"motivate\",\"retention\",\"hire\"]\n",
    "\n",
    "\n",
    "##  exclude keyword list\n",
    "exlude_keywords = [\"customer\",\"customers\",\"supplier\",\"suppliers\",\"client\",\"clients\",\"contract\",\"contracts\",\n",
    "                \"creditor\",\"creditors\",\"investor\",\"investors\",\"business\",\"businesses\",\"segments\",\"segment\",\n",
    "                \"subscriber\",\"subscribers\",\"right\",\"rights\"]\n",
    "##　negative prefix list\n",
    "negativePrefix = ['un', 'in', 'im','il','ir','non', 'mis','mal','dis','anti','de','under', 'semi', 'mini', 'ex', 'sub', 'infra']\n",
    "\n",
    "##  keywords list combination (just for keywords1 - 2)\n",
    "key_combination_list = ['^(?=.*skill).*$']\n",
    "'''\n",
    "for k1 in keywords:\n",
    "    for k2 in keywords2:\n",
    "        #https://newbedev.com/regex-i-want-this-and-that-and-that-in-any-order\n",
    "        temp = '^(?=.*'+k1+')(?=.*'+k2+').*$'\n",
    "        key_combination_list.append(temp)\n",
    "'''\n",
    "for k1 in keywords:\n",
    "    for k2 in keywords2:\n",
    "        #temp = \" \".join([k1,k2])\n",
    "        #k2+'.*?'+k3\n",
    "        temp = '^(?=.*'+k1+'.*?'+k2+').*$'\n",
    "        key_combination_list.append(temp)\n",
    "for k1 in keywords:\n",
    "    for k2 in keywords2:\n",
    "        temp = \" \".join([k1,k2])\n",
    "        #k2+'.*?'+k3\n",
    "        temp = '^(?=.*'+temp+').*$'\n",
    "        key_combination_list.append(temp)\n",
    "\n",
    "\n",
    "##  keywords list combination (for keywords3)\n",
    "key_combination_list_for_keyword3 = [] \n",
    "for k3 in keywords3:\n",
    "    temp = '^(?=.*'+k3+').*$'\n",
    "    key_combination_list_for_keyword3.append(temp)\n",
    "\n",
    "for k1 in keywords4:\n",
    "    for k2 in keywords4:\n",
    "        if k1!= k2:\n",
    "            temp = k1+'.*?'+\"and \"+k2\n",
    "            key_combination_list_for_keyword3.append(temp)\n",
    "            temp = k1+'.*?'+\"or \"+k2\n",
    "            key_combination_list_for_keyword3.append(temp)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "for k1 in keywords4:\n",
    "    for k2 in keywords4:\n",
    "        if k1!= k2:\n",
    "            temp = k1+'.*?'+\", \"+k2\n",
    "            key_combination_list_for_keyword3.append(temp)\n",
    "        else:\n",
    "            pass\n",
    "##  exclude  keywords list combination \n",
    "#https://learnbyexample.github.io/python-regex-cheatsheet/\n",
    "\n",
    "##  exclude  keywords list combination (just for keywords1 negative prefix)\n",
    "exclude_key_combination_list = ['^(?=.*design).*$']\n",
    "for k1 in negativePrefix:\n",
    "    for k2 in keywords:\n",
    "        temp = \"\".join([k1,k2])\n",
    "        temp = '^(?=.*'+temp+').*$'\n",
    "        temp1 = \"-\".join([k1,k2])\n",
    "        temp1 = '^(?=.*'+temp1+').*$'\n",
    "        exclude_key_combination_list.append(temp)\n",
    "        exclude_key_combination_list.append(temp1)\n",
    "\n",
    "##  exclude  keywords list combination (just for keywords3)\n",
    "exclude_key_combination_list_for_keywords3 = ['^(?=.*design).*$']\n",
    "for k1 in keywords3:\n",
    "    for k2 in exlude_keywords:\n",
    "        temp = k1+'.*?'+k2\n",
    "        exclude_key_combination_list_for_keywords3.append(temp)\n",
    "\n",
    "for k1 in keywords4:\n",
    "    for k2 in keywords4:\n",
    "        for k3 in exlude_keywords:\n",
    "            if k1!= k2:\n",
    "                temp = k1+'.*?'+\"and \"+k2+'.*?'+k3\n",
    "                exclude_key_combination_list_for_keywords3.append(temp)\n",
    "                temp = k1+'.*?'+\"or \"+k2+'.*?'+k3\n",
    "                exclude_key_combination_list_for_keywords3.append(temp)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "for k1 in negativePrefix:\n",
    "    for k2 in keywords3:\n",
    "        temp = \"\".join([k1,k2])\n",
    "        temp = '^(?=.*'+temp+').*$'\n",
    "        temp1 = \"-\".join([k1,k2])\n",
    "        temp1 = '^(?=.*'+temp1+').*$'\n",
    "        exclude_key_combination_list_for_keywords3.append(temp)\n",
    "        exclude_key_combination_list_for_keywords3.append(temp1)\n",
    "\n",
    "##  keywords product list combination \n",
    "key_product_combination_list = []\n",
    "for k2 in keywords2:\n",
    "    #https://www.regextester.com/15\n",
    "    temp = '(?=.*product)^((?!'+k2+').)*$'\n",
    "    key_product_combination_list.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "############################################  count keywords    ############################################\n",
    "## count how many sentences containing specific keywords  \n",
    "def count_keywords_function(sentence_list,keywords_type):\n",
    "    temp  = []\n",
    "    for sentence in sentence_list :\n",
    "        temp.append((sum(1 for \n",
    "        word in keywords_type if len(re.findall(word, sentence))==1)))\n",
    "    return temp\n",
    "\n",
    "def extract_sentence_function(sentence_list,keywords_location_list,negative):\n",
    "\n",
    "    temp_sentences  = []\n",
    "    \n",
    "    if negative == 1:\n",
    "        for location in [i for i in range(len(keywords_location_list)) if keywords_location_list[i] == 0]:\n",
    "            #print(location)\n",
    "            temp_sentences.append(sentence_list[location])\n",
    "            #print(sentence_list[location])\n",
    "    else:\n",
    "        for location in [i for i in range(len(keywords_location_list)) if keywords_location_list[i] > 0]:\n",
    "            #print(location)\n",
    "            if len(sentence_list[location].split())>20:\n",
    "                temp_sentences.append(sentence_list[location])\n",
    "            #print(sentences[location])\n",
    "        \n",
    "    return temp_sentences\n",
    "\n",
    "def extract_sentence_function_for_design(sentence_list,negative):\n",
    "    temp_sentences  = []\n",
    "    keywords_location_list  = []\n",
    "    keywords_type_list = ['^(?=.*design.*?employee).*$','^(?=.*design.*?talent).*$']\n",
    "    for sentence in sentence_list :\n",
    "        keywords_location_list.append((sum(1 for word in keywords_type_list if len(re.findall(word, sentence))==1)))\n",
    "    if negative == 1:\n",
    "        for location in [i for i in range(len(keywords_location_list)) if keywords_location_list[i]>0]:\n",
    "            #print(location)\n",
    "            temp_sentences.append(sentence_list[location])\n",
    "            #print(sentence_list[location])\n",
    "    else:\n",
    "        pass\n",
    "    return temp_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第幾個 58 \n",
      "哪家公司 APACHE OFFSHORE INVESTMENT PARTNERSHIP\n",
      "APA1997.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA1998.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA1999.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2000.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2001.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2002.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2003.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2004.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2005.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2006.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2007.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2008.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2009.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2010.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2011.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2012.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2013.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2014.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2015.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2016.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2018.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APA2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sandy\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "parent_dir = \"D:/Stata/stata 桌面/stata系列/計畫18_又予/20210104新計畫/20210128_重新整理/以sp1500為主(新的)/stata/10-K/10-K報表\"\n",
    "import pdftotext\n",
    "for z in range(58,59):\n",
    "    print(\"第幾個\",z,\"\\n哪家公司\",company_list['company name'].iloc[z])\n",
    "    parent_dir = \"D:/Stata/stata 桌面/stata系列/計畫18_又予/20210104新計畫/20210128_重新整理/以sp1500為主(新的)/stata/10-K/10-K報表\"\n",
    "    directory = str(company_list['tic'].iloc[z])\n",
    "    path = os.path.join(parent_dir, directory) \n",
    "    try:\n",
    "        os.chdir(path)\n",
    "        list_in_file=os.listdir(path)\n",
    "        #list_in_file = [int(x.split(str(company_list['company name'].iloc[z]))[1].split('.pdf')[0]) for x in list_in_file]\n",
    "\n",
    "        ############################################  read pdf  ############################################\n",
    "        ## read package\n",
    "        import pdftotext\n",
    "        from six.moves.urllib.request import urlopen\n",
    "        skill_labor_risk_company = pd.DataFrame()\n",
    "        error_year = []\n",
    "        #max_lst = max(list_in_file)+1\n",
    "        for i in os.listdir(path):\n",
    "            pdf_year=i\n",
    "            print(pdf_year)\n",
    "            try:\n",
    "                ## open pdf\n",
    "                with open(pdf_year , \"rb\") as f:\n",
    "                    pdf = pdftotext.PDF(f)\n",
    "            except:\n",
    "                pass\n",
    "            ## merge all in a list\n",
    "            paragraph = [] \n",
    "            for page in pdf:\n",
    "                paragraph.append(page)\n",
    "            # as per recommendation from @freylis, compile once only\n",
    "            ##  clean string \n",
    "            CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') \n",
    "            def cleanhtml(raw_html):\n",
    "                cleantext = re.sub(CLEANR, '', raw_html)\n",
    "                return cleantext\n",
    "            ## turn to lower    \n",
    "            paragraph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph]\n",
    "            paragraph = [cleanhtml(x) for x in paragraph]\n",
    "\n",
    "\n",
    "\n",
    "            ############################################  Deal with Item paragraph  ############################################\n",
    "            ##  find Item paragraph\n",
    "            try:\n",
    "                pdf_year = i.split(str(company_list['company name'].iloc[z]))[1]\n",
    "            except:\n",
    "                pdf_year = i.split(str(company_list['tic'].iloc[z]))[1]\n",
    "            \n",
    "            try:\n",
    "                # item 1.\n",
    "                temp = [x.find('item 1. ') for x in paragraph]\n",
    "                # item 2. or item 1a.\n",
    "                if int(pdf_year.split(\".\")[0])<=2004:\n",
    "                    temp2 = [x.find('item 2. ') for x in paragraph]\n",
    "                    \n",
    "                else:\n",
    "                    temp2 = [x.find('item 1a. ') for x in paragraph]\n",
    "                    temp2_plus = [x.find('item 2. ') for x in paragraph]\n",
    "                # item 3.\n",
    "                temp3 = [x.find('item 7. ') for x in paragraph]\n",
    "                # item 4.\n",
    "                temp4 = [x.find('item 8. ') for x in paragraph]\n",
    "            except:\n",
    "                # item 1.\n",
    "                temp = [x.find('item1. ') for x in paragraph]\n",
    "                # item 2. or item 1a.\n",
    "                if int(pdf_year.split(\".\")[0])<=2004:\n",
    "                    temp2 = [x.find('item2. ') for x in paragraph]\n",
    "                    \n",
    "                else:\n",
    "                    temp2 = [x.find('item1a. ') for x in paragraph]\n",
    "                    temp2_plus = [x.find('item2. ') for x in paragraph]\n",
    "                # item 3.\n",
    "                temp3 = [x.find('item7. ') for x in paragraph]\n",
    "                # item 4.\n",
    "                temp4 = [x.find('item8. ') for x in paragraph]\n",
    "\n",
    "\n",
    "            \n",
    "            # split paragraph - item 1 & item 1a\n",
    "            if int(pdf_year.split(\".\")[0])<=2004:\n",
    "                try :\n",
    "                    item1_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp.index([x for x in temp if x != -1][0]):temp2.index([x for x in temp2 if x != -1][0])]\n",
    "                except:\n",
    "                    error_year.append(pdf_year.split(\".\")[0])\n",
    "            else:\n",
    "                try:    \n",
    "                    try:\n",
    "                        item1_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp.index([x for x in temp if x != -1][1]):temp2.index([x for x in temp2 if x != -1][1])]\n",
    "                        item1a_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp2.index([x for x in temp2 if x != -1][1]):temp2_plus.index([x for x in temp2_plus if x != -1][1])]\n",
    "                        item1_paragrph.extend(item1a_paragrph)\n",
    "                    except (IndexError):\n",
    "                        try:\n",
    "                            item1_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp.index([x for x in temp if x != -1][0]):temp2_plus.index([x for x in temp2_plus if x != -1][0])]\n",
    "                        except:\n",
    "                            error_year.append(pdf_year.split(\".\")[0])\n",
    "                except:\n",
    "                    try:\n",
    "                        item1_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp.index([x for x in temp if x != -1][0]):temp2.index([x for x in temp2 if x != -1][0])]\n",
    "                        item1a_paragrph = [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp2.index([x for x in temp2 if x != -1][0]):temp2_plus.index([x for x in temp2_plus if x != -1][0])]\n",
    "                        item1_paragrph.extend(item1a_paragrph)\n",
    "                    except:\n",
    "                        error_year.append(pdf_year.split(\".\")[0])      \n",
    "            try:      \n",
    "                # split paragraph - item 7\n",
    "                item7_paragrph= [x.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \") for x in paragraph][temp3.index([x for x in temp3 if x != -1][0]):temp4.index([x for x in temp4 if x != -1][0])]\n",
    "\n",
    "                # merge  list\n",
    "                item1_paragrph.extend(item7_paragrph)\n",
    "                # one paragraph\n",
    "                item1_paragrph = ' '.join(item1_paragrph)\n",
    "            except:\n",
    "                error_year.append(pdf_year.split(\".\")[0])\n",
    "\n",
    "            ############################################  using lemmatize_sentence to transform paragraph  ############################################\n",
    "            from pywsd.utils import lemmatize_sentence\n",
    "            # split one paragraph into sentences\n",
    "            nltk.download('punkt')\n",
    "            sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            #sentences =  item1_paragrph.split(\". \")\n",
    "            #sentences = [x.replace(\"”\",\"\") for  x in sentences]\n",
    "            try:\n",
    "                sentences = sent_segmenter.tokenize(item1_paragrph)\n",
    "                sentences = [' '.join(lemmatize_sentence(y)) for y in sentences]\n",
    "                sentences = [' '.join(y.split(\";\")) for y in sentences]\n",
    "            except:\n",
    "                #sentences = [str(y) for y in item1_paragrph]\n",
    "                item1_paragrph = [sent_segmenter.tokenize(str(y)) for y in item1_paragrph]\n",
    "                sentences = sum(item1_paragrph, [])\n",
    "                sentences = [' '.join(lemmatize_sentence(y)) for y in sentences]\n",
    "                sentences = [' '.join(y.split(\";\")) for y in sentences]\n",
    "                pass\n",
    "        \n",
    "\n",
    "            ############################################  count keywords    ############################################\n",
    "\n",
    "            ## without excluding some keywords (keywords1 - 2 skill labor risk)\n",
    "            temp = count_keywords_function(sentences,key_combination_list)\n",
    "            temp_sentences = extract_sentence_function(sentences,temp,0)\n",
    "\n",
    "            ## exclude some unnecessary keywords (keywords1 - 2 skill labor risk)\n",
    "            temp = count_keywords_function(temp_sentences,exclude_key_combination_list)\n",
    "            temp_sentences_exclude = extract_sentence_function(temp_sentences,temp,1)\n",
    "            temp_sentences_exclude_design = extract_sentence_function_for_design(temp_sentences,1)\n",
    "            temp_sentences_exclude.extend(temp_sentences_exclude_design)\n",
    "            #temp = count_keywords_function(temp_sentences_exclude ,exclude_key_combination_list_for_keywords3)\n",
    "            #temp_sentences_exclude = extract_sentence_function(temp_sentences_exclude ,temp,1)\n",
    "\n",
    "            ## without excluding some keywords (keywords 3 skill labor risk)\n",
    "            temp_keywords3 = count_keywords_function(sentences,key_combination_list_for_keyword3)\n",
    "            temp_sentences_keywords3 = extract_sentence_function(sentences,temp_keywords3 ,0)\n",
    "\n",
    "            ## exclude some unnecessary keywords (keywords 3 skill labor risk)\n",
    "            temp_keywords3 = count_keywords_function(temp_sentences_keywords3,exclude_key_combination_list_for_keywords3)\n",
    "            temp_sentences_exclude_keywords3 = extract_sentence_function(temp_sentences_keywords3,temp_keywords3,1)\n",
    "        \n",
    "            #temp_keywords3 = count_keywords_function(temp_sentences_exclude_keywords3 ,exclude_key_combination_list)\n",
    "            #temp_sentences_exclude_keywords3 = extract_sentence_function(temp_sentences_exclude_keywords3,temp_keywords3,1)\n",
    "\n",
    "\n",
    "            ## combine keyword1、2、3 skill labor risk\n",
    "            temp_sentences_exclude.extend(temp_sentences_exclude_keywords3)\n",
    "\n",
    "            #temp_keywords_1 = count_keywords_function(temp_sentences_exclude ,exclude_key_combination_list)\n",
    "            #temp_sentences_exclude = extract_sentence_function(temp_sentences_exclude,temp_keywords_1,1)\n",
    "\n",
    "            ## find sentence where contain \"product\" and exclude \"labor、employee....\" which mentioned people\n",
    "            temp = count_keywords_function(temp_sentences_exclude,key_product_combination_list)\n",
    "            temp = [1 if x >0 else 0  for x in temp ]\n",
    "\n",
    "            ############################################  combine to be a dataframe    ############################################\n",
    "            if temp == [] :\n",
    "                try:\n",
    "                    df=pd.DataFrame([[],[],[0],[len(sentences)],[str(i.split(str(company_list['company name'].iloc[z]))[1].split('.pdf')[0])]]).T\n",
    "                except:\n",
    "                    df=pd.DataFrame([[],[],[0],[len(sentences)],[str(i.split(str(company_list['tic'].iloc[z]))[1].split('.pdf')[0])]]).T\n",
    "                df.columns = [\"skill_labo_risk_sentence\",\"sentence_not_mention_people\",\"total_sentence_mention_skill_labor_risk\",\"total_sentence\",\"Year\"]\n",
    "            else:\n",
    "                try:\n",
    "                    df=pd.DataFrame([temp_sentences_exclude,temp,[len(temp)]*len(temp),[len(sentences)]*len(temp),[str(i.split(str(company_list['company name'].iloc[z]))[1].split('.pdf')[0])]*len(temp)]).T\n",
    "                except:\n",
    "                    df=pd.DataFrame([temp_sentences_exclude,temp,[len(temp)]*len(temp),[len(sentences)]*len(temp),[str(i.split(str(company_list['tic'].iloc[z]))[1].split('.pdf')[0])]*len(temp)]).T\n",
    "                df.columns = [\"skill_labo_risk_sentence\",\"sentence_not_mention_people\",\"total_sentence_mention_skill_labor_risk\",\"total_sentence\",\"Year\"]\n",
    "\n",
    "\n",
    "            skill_labor_risk_company =  skill_labor_risk_company.append(df, ignore_index=True)\n",
    "\n",
    "        skill_labor_risk_company.drop_duplicates(subset=['skill_labo_risk_sentence', 'Year'], keep=\"first\",inplace=True)\n",
    "        os.chdir(\"D:\\\\Stata\\\\stata 桌面\\\\stata系列\\\\計畫18_又予\\\\20210104新計畫\\\\20210128_重新整理\\\\以sp1500為主(新的)\\\\stata\\\\10-K\\\\10-k skill_labor_risk\\\\20220207\")\n",
    "        skill_labor_risk_company.to_csv(\"skill_labor_risk_\"+str(company_list['tic'].iloc[z])+\"_20220126.csv\",encoding='utf-8-sig', index=False)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_labo_risk_sentence</th>\n",
       "      <th>sentence_not_mention_people</th>\n",
       "      <th>total_sentence_mention_skill_labor_risk</th>\n",
       "      <th>total_sentence</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the time have collective bargaining agreement ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the time have collective bargaining agreement ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>compensation and benefit we offer a comprehens...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>374</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>our ability to attract and retain our user be ...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>374</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>in addition , we have implement and may contin...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>374</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>these dynamic have affect , and will likely co...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>374</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>in addition , we must continue to offer compet...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>374</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              skill_labo_risk_sentence  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2                                                 None   \n",
       "3    the time have collective bargaining agreement ...   \n",
       "4    the time have collective bargaining agreement ...   \n",
       "..                                                 ...   \n",
       "116  compensation and benefit we offer a comprehens...   \n",
       "117  our ability to attract and retain our user be ...   \n",
       "118  in addition , we have implement and may contin...   \n",
       "119  these dynamic have affect , and will likely co...   \n",
       "122  in addition , we must continue to offer compet...   \n",
       "\n",
       "    sentence_not_mention_people total_sentence_mention_skill_labor_risk  \\\n",
       "0                          None                                       0   \n",
       "1                          None                                       0   \n",
       "2                          None                                       0   \n",
       "3                             1                                       1   \n",
       "4                             1                                       1   \n",
       "..                          ...                                     ...   \n",
       "116                           0                                      21   \n",
       "117                           1                                      21   \n",
       "118                           0                                      21   \n",
       "119                           0                                      21   \n",
       "122                           0                                      21   \n",
       "\n",
       "    total_sentence  Year  \n",
       "0              178  1993  \n",
       "1              161  1994  \n",
       "2              170  1995  \n",
       "3              197  1996  \n",
       "4              182  1997  \n",
       "..             ...   ...  \n",
       "116            374  2020  \n",
       "117            374  2020  \n",
       "118            374  2020  \n",
       "119            374  2020  \n",
       "122            374  2020  \n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_labor_risk_company"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
